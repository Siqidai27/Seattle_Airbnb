set.seed(1)
km.out=kmeans(x,4,nstart=1)
km.out
plot(x, col=km.out$cluster, pch=1, cex=2,lwd=2)
points(x, col=c(3,1,4,2)[which], pch=19)
set.seed(1)
km.out=kmeans(x,4,nstart=20)
km.out
plot(x, col=km.out$cluster, pch=1, cex=2,lwd=2)
points(x, col=c(3,1,4,2)[which], pch=19)
# hierarchical clustering
hc.complete=hclust(dist(x), method="complete")
hc.average=hclust(dist(x), method="average")
hc.single=hclust(dist(x), method="single")
plot(hc.complete,main="Complete Linkage", cex=.9)
plot(hc.average, main="Average Linkage", cex=.9)
plot(hc.single, main="Single Linkage", cex=.9)
hc.cut = cutree(hc.complete,4)
table(hc.cut,which)
plot(x, col=hc.cut, pch=1, cex=2,lwd=2)
# generate a 3-dim example data set
x3=matrix(rnorm(30*3), ncol=3)
dd=as.dist(1-cor(t(x3)))
plot(hclust(dd, method="complete"))
# use NC160 data set
nci.labs=NCI60$labs
nci.data=NCI60$data
sd.data=scale(nci.data)
# perform hierarchical clustering and plot them
hc.complete1=hclust(dist(sd.data), method="complete")
hc.average1=hclust(dist(sd.data), method="average")
hc.single1=hclust(dist(sd.data), method="single")
plot(hc.complete1,main="Complete Linkage", cex=.9)
plot(hc.average1, main="Average Linkage", cex=.9)
plot(hc.single1, main="Single Linkage", cex=.9)
# cutting dendrogram to yield 5 clusters
hc.cut1 = cutree(hc.complete1,5)
table(nci.labs, hc.cut1)
# kmeans VS hierarchical clustering
set.seed(3)
km.out1=kmeans(sd.data,5,nstart=20)
km.out1
table(km.out1$cluster,hc.cut1)
# correlated distance VS euclidean distance
dd1=as.dist(1-cor(t(sd.data)))
hc_cor = hclust(dd1, method="complete")
hc.cut_cor = cutree(hc_cor,5)
# complete with correlated distance VS complete with euclidean distance
table(hc.cut_cor, hc.cut1)
# kmeans VS complete linkage with correlated distance
table(hc.cut_cor, km.out1$cluster)
library(ISLR)
set.seed(101)
x=matrix(rnorm(100*2),100,2)
xmean=matrix(rnorm(8,sd=4),4,2)
which=sample(1:4,100,replace=TRUE)
x=x+xmean[which,]
plot(x,col=which,pch=19)
set.seed(4)
km.out=kmeans(x,4,nstart=15)
km.out
plot(x, col=km.out$cluster, pch=1, cex=2,lwd=2)
points(x, col=c(3,1,4,2)[which], pch=19)
set.seed(1)
km.out=kmeans(x,4,nstart=1)
km.out
plot(x, col=km.out$cluster, pch=1, cex=2,lwd=2)
points(x, col=c(3,1,4,2)[which], pch=19)
set.seed(1)
km.out=kmeans(x,4,nstart=20)
km.out
plot(x, col=km.out$cluster, pch=1, cex=2,lwd=2)
points(x, col=c(3,1,4,2)[which], pch=19)
install.packages("splines")
library(ISLR)
attach(Wage)
library(splines)
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
install.packages("gam")
install.packages("gam")
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
library(ISLR)
attach(Wage)
library(splines)
library(gam)
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
library(ISLR)
attach(Wage)
library(splines)
library(gam)
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
par(mfrow = c(1, 3))
plot(gam.m3, se=TRUE,col="blue")
plot.gam(gam1, se=TRUE, col="red")
library(ISLR)
attach(Wage)
library(splines)
library(gam)
# GAM with natural spline and smooth spline
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
par(mfrow = c(1, 3))
plot(gam.m3, se=TRUE,col="blue")
plot.gam(gam1, se=TRUE, col="red")
# Model selection using ANOVA
gam.m1=gam(wage~s(age,5)+education,data=Wage)
gam.m2=gam(wage~year+s(age,5)+education,data=Wage)
anova(gam.m1, gam.m2, gam.m3, test="F")
summary(gam.m3)
summary(gam.m3)
preds=predict(gam.m2, newdata=Wage)
preds=predict(gam.m2, newdata=Wage)
View(preds)
# GAM for Classification Problems
gam.lr=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="green")
table(education,I(wage>250))
gam.lr=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
plot(gam.lr, se = T, col = "pink")
summary(gam.m3)
summary(gam.m2)
gam.lr=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
plot(gam.lr, se = T, col = "pink")
gam.m4=gam(wage~poly(year, 6)+bs(age,5)+education,data=Wage)
m1 = gam(I(wage>250)~year+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
m2 = gam(I(wage>250)~year+age+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
m3 = gam(I(wage>250)~year+s(age,2)+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
m4 = gam(I(wage>250)~year+s(age,5)+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
m5 = gam(I(wage>250)~year+s(age,8)+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
anova(m1, m2, m3, m4, m5, test="F")
help("anova")
anova(m1, m2, m3, m4, m5)
library(ISLR)
attach(Wage)
library(splines)
library(gam)
# GAM with natural spline and smooth spline
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
par(mfrow = c(1, 3))
plot(gam.m3, se=TRUE,col="blue")
plot.gam(gam1, se=TRUE, col="red")
# Model selection using ANOVA
gam.m1=gam(wage~s(age,5)+education,data=Wage)
gam.m2=gam(wage~year+s(age,5)+education,data=Wage)
anova(gam.m1, gam.m2, gam.m3, test="F")
summary(gam.m3)
summary(gam.m2)
preds=predict(gam.m2, newdata=Wage)
# GAM for Classification Problems
gam.lr=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="green")
table(education,I(wage>250))
# GAM excluding category < HS
gam.lr=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
plot(gam.lr, se = T, col = "pink")
# GAM with specific form
gam.m4=gam(wage~poly(year, 6)+bs(age,5)+education,data=Wage)
# Take-home questions
m1 = gam(I(wage>250)~year+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
m2 = gam(I(wage>250)~year+age+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
m3 = gam(I(wage>250)~year+s(age,2)+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
m4 = gam(I(wage>250)~year+s(age,5)+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
m5 = gam(I(wage>250)~year+s(age,8)+education,family=binomial,data=Wage,
subset=(education!="1. < HS Grad") )
anova(m1, m2, m3, m4, m5, test="F")
install.packages("tree")
library(tree)
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
View(Carseats)
tree.carseats=tree(High∼.-Sales ,Carseats )
summary(tree.carseats)
tree.carseats=tree(High∼.-Sales ,Carseats)
tree.carseats=tree(High~.-Sales ,Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
summary(fit)
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
set.seed(3)
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
cv.carseats
prune.carseats=prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
install.packages("MASS")
install.packages("randomForest")
library(MASS)
library(randomForest)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,
importance=TRUE)
bag.boston
set.seed(1)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
library(MASS)
library(randomForest)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,
importance=TRUE)
bag.boston
#random forest
library(MASS)
library(randomForest)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,
importance=TRUE)
bag.boston
set.seed(1)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat.bag, boston.test)
abline(0,1)
#calculate MSE
mean((yhat.bag-boston.test)^2)
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
yhat.bag = predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,
importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,
importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,
importance=TRUE)
yhat.rf = predict(rf.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
set.seed(2)
train=sample(1:nrow(Carseats), 200)
View(Carseats.test)
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
View(corollas)
View(Carseats.test)
library(tree)
library(ISLR)
attach(Carseats)
library(randomForest)
# Create qualitative response "High"
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
# Generate training set and test set of equal sizes
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
# Bagging with 10 Trees
set.seed(2)
bag.carseats=randomForest(High~.-Sales,data=Carseats,subset=train,mtry=10,
ntree=10,importance=TRUE)
bag.carseats
# Calculate test classification error rate
High.pred=predict(bag.carseats,Carseats.test)
table(High.pred,High.test)
importance(bag.carseats)
varImpPlot(bag.carseats)
library(tree)
library(ISLR)
attach(Carseats)
library(randomForest)
# Create qualitative response "High"
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
# Generate training set and test set of equal sizes
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
set.seed(2)
bag.carseats=randomForest(High~.-Sales,data=Carseats,subset=train,mtry=10,
ntree=10,importance=TRUE)
bag.carseats
High.pred=predict(bag.carseats,Carseats.test)
table(High.pred,High.test)
importance(bag.carseats)
varImpPlot(bag.carseats)
set.seed(2)
bag.carseats=randomForest(High~.-Sales,data=Carseats,subset=train,mtry=10,
ntree=500,importance=TRUE)
bag.carseats
High.pred = predict(bag.carseats,Carseats.test)
table(High.pred,High.test)
# Bagging with 500 trees and 3 random features
set.seed(2)
bag.carseats=randomForest(High~.-Sales,data=Carseats,subset=train,mtry=3,
ntree=500,importance=TRUE)
bag.carseats
# Calculate test classification error rate
High.pred = predict(bag.carseats,Carseats.test)
table(High.pred,High.test)
importance(bag.carseats)
varImpPlot(bag.carseats)
View(Carseats)
library(tree)
library(ISLR)
attach(Carseats)
library(randomForest)
# Create qualitative response "High"
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
# Generate training set and test set of equal sizes
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
# Bagging with 10 Trees
set.seed(2)
bag.carseats=randomForest(High~.-Sales,data=Carseats,subset=train,mtry=10,
ntree=10,importance=TRUE)
bag.carseats
# Calculate test classification error rate
High.pred=predict(bag.carseats,Carseats.test)
table(High.pred,High.test)
importance(bag.carseats)
varImpPlot(bag.carseats)
View(Carseats)
library(tree)
library(ISLR)
attach(Carseats)
library(randomForest)
# Create qualitative response "High"
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
# Generate training set and test set of equal sizes
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
# Bagging with 10 Trees
set.seed(2)
bag.carseats=randomForest(High~.-Sales,data=Carseats,subset=train,mtry=10,
ntree=10,importance=TRUE)
bag.carseats
# Calculate test classification error rate
High.pred=predict(bag.carseats,Carseats.test)
table(High.pred,High.test)
importance(bag.carseats)
varImpPlot(bag.carseats)
# Bagging with 500 trees
set.seed(2)
bag.carseats=randomForest(High~.-Sales,data=Carseats,subset=train,mtry=10,
ntree=500,importance=TRUE)
bag.carseats
# Calculate test classification error rate
High.pred = predict(bag.carseats,Carseats.test)
table(High.pred,High.test)
library(leaps)
regfit.bwd=regsubsets(price~.,data = listings, nvmax = 43, method = "backward")
summary(regfit.full)
setwd("/Users/siyuan/Documents/Spring 2017/Orie 4740 - Data Mining/Seattle_Airbnb")
listings = read.csv("data/listings.csv", header = TRUE)
# Remove arbitrary features
listings[c("thumbnail_url", "medium_url", "picture_url", "xl_picture_url",
"host_url", "host_thumbnail_url", "host_picture_url", "host_neighbourhood",
"listing_url", "experiences_offered", "id", "scrape_id", "last_scraped", "name", "host_id",
"host_name", "host_since", "host_location", "host_acceptance_rate", "host_verifications",
"host_has_profile_pic", "host_identity_verified", "street", "neighbourhood",
"city", "state", "market", "zipcode", "smart_location", "country_code", "country",
"is_location_exact", "square_feet", "weekly_price", "monthly_price", "maximum_nights",
"calendar_updated", "has_availability", "calendar_last_scraped", "first_review", "last_review",
"requires_license", "jurisdiction_names", "license", "neighbourhood_cleansed")] = NULL
# Remove dollar sign and percentage in price-related features and convert them to number
listings$security_deposit = as.numeric(gsub("[\\$,]", "", listings$security_deposit))
listings$price = as.numeric(gsub("[\\$,]", "", listings$price))
listings$cleaning_fee = as.numeric(gsub("[\\$,]", "", listings$cleaning_fee))
listings$extra_people = as.numeric(gsub("[\\$,]", "", listings$extra_people))
listings$host_response_rate = as.numeric(gsub("[,\\%]", "", listings$host_response_rate))
# Replace missing value in features with 0
listings$security_deposit[is.na(listings$security_deposit)] = 0
listings$cleaning_fee[is.na(listings$cleaning_fee)] = 0
listings$reviews_per_month[is.na(listings$reviews_per_month)] = 0
listings$bathrooms[is.na(listings$bathrooms)] = 0
listings$bedrooms[is.na(listings$bedrooms)] = 0
listings$beds[is.na(listings$beds)] = 0
# Replace missing values in feature with average of the feature
listings$review_scores_rating[is.na(listings$review_scores_rating)] = mean(listings$review_scores_rating, na.rm = TRUE)
listings$review_scores_accuracy[is.na(listings$review_scores_accuracy)] = mean(listings$review_scores_accuracy, na.rm = TRUE)
listings$review_scores_cleanliness[is.na(listings$review_scores_cleanliness)] = mean(listings$review_scores_cleanliness, na.rm = TRUE)
listings$review_scores_checkin[is.na(listings$review_scores_checkin)] = mean(listings$review_scores_checkin, na.rm = TRUE)
listings$review_scores_communication[is.na(listings$review_scores_communication)] = mean(listings$review_scores_communication, na.rm = TRUE)
listings$review_scores_location[is.na(listings$review_scores_location)] = mean(listings$review_scores_location, na.rm = TRUE)
listings$review_scores_value[is.na(listings$review_scores_value)] = mean(listings$review_scores_value, na.rm = TRUE)
listings$host_response_rate[is.na(listings$host_response_rate)] = mean(listings$host_response_rate, na.rm = TRUE)
# Sum number of words in text-related features
listings$count_summary = sapply(gregexpr("[[:alpha:]]+", listings$summary), function(x) sum(x > 0))
listings$count_summary = listings$count_summary + sapply(gregexpr("[[:digit:]]+\\.*[[:digit:]]*", listings$summary), function(x) sum(x > 0))
listings$count_description = sapply(gregexpr("[[:alpha:]]+", listings$description), function(x) sum(x > 0))
listings$count_description = listings$count_description + sapply(gregexpr("[[:digit:]]+\\.*[[:digit:]]*", listings$description), function(x) sum(x > 0))
listings$count_space = sapply(gregexpr("[[:alpha:]]+", listings$space), function(x) sum(x > 0))
listings$count_space = listings$count_space + sapply(gregexpr("[[:digit:]]+\\.*[[:digit:]]*", listings$space), function(x) sum(x > 0))
listings$count_neighborhood = sapply(gregexpr("[[:alpha:]]+", listings$neighborhood_overview), function(x) sum(x > 0))
listings$count_neighborhood = listings$count_neighborhood + sapply(gregexpr("[[:digit:]]+\\.*[[:digit:]]*", listings$neighborhood_overview), function(x) sum(x > 0))
listings$count_notes = sapply(gregexpr("[[:alpha:]]+", listings$notes), function(x) sum(x > 0))
listings$count_notes = listings$count_notes + sapply(gregexpr("[[:digit:]]+\\.*[[:digit:]]*", listings$notes), function(x) sum(x > 0))
listings$count_transit = sapply(gregexpr("[[:alpha:]]+", listings$transit), function(x) sum(x > 0))
listings$count_transit = listings$count_transit + sapply(gregexpr("[[:digit:]]+\\.*[[:digit:]]*", listings$transit), function(x) sum(x > 0))
listings$count_host = sapply(gregexpr("[[:alpha:]]+", listings$host_about), function(x) sum(x > 0))
listings$count_host = listings$count_host + sapply(gregexpr("[[:digit:]]+\\.*[[:digit:]]*", listings$host_about), function(x) sum(x > 0))
listings$count_amenities = sapply(gregexpr("[^,\\s][^\\,]*[^,\\s]*", listings$amenities), function(x) sum(x > 0))
# Replace missing value cannot be selected by regular expression with 0
listings$count_amenities[which(listings$amenities == "{}")] = 0
listings$count_notes[which(is.na(listings$notes))] = 0
# Calculate the total number of words across text-related features
listings$count_total = listings$count_summary + listings$count_description + listings$count_space +
listings$count_neighborhood + listings$count_notes + listings$count_transit + listings$count_host
# Remove redundant feautures
listings[c("summary", "description", "space", "amenities", "neighborhood_overview",
"notes", "transit", "host_about")] = NULL
# check for missing values in listings
unique (unlist(lapply (listings, function (x) which (is.na (x)))))
# Construct a new data frame for modeling
new_listings = listings
new_listings[c("count_notes","count_description","count_space","count_transit",
"count_host","count_neighborhood")] = NULL
sapply(new_listings,class)
train_ind <- sample(1:nrow(new_listings), 2/3*nrow(new_listings))
listings.train <- new_listings[train_ind, ]
listings.test <- new_listings[-train_ind, ]
library(leaps)
regfit.bwd=regsubsets(price~.,data = listings, nvmax = 43, method = "backward")
summary(regfit.full)
library(leaps)
regfit.bwd=regsubsets(price~.,data = listings, nvmax = 43, method = "backward")
summary(regfit.bwd)
summary(regfit.bwd)$adjr2
library(leaps)
regfit.bwd=regsubsets(price~.,data = listings, nvmax = 10, method = "backward")
summary(regfit.bwd)$adjr2
summary(regfit.bwd)
x = model.matrix(price~., listings)
lasso.fit = glmnet(x, listings$price, alpha = 1, lambda = 0.5)
install.packages("glmnet")
library(glmnet)
x = model.matrix(price~., listings)
lasso.fit = glmnet(x, listings$price, alpha = 1, lambda = 0.5)
lasso.fit = glmnet(listings.train, listings.train$price, alpha = 1, lambda = grid)
lasso.fit = glmnet(listings.train, listings.train$price, alpha = 1, lambda = 2)
plot(lasso.fit)
lasso.fit = glmnet(listings.train, alpha = 1, lambda = 2)
plot(lasso.fit)
lasso.fit = glmnet(listings.train, listings.train$price, alpha = 1, lambda = 2)
plot(lasso.fit)
x = model.matrix(price~., listings.train)
lasso.fit = glmnet(x, listings.train$price, alpha = 1, lambda = grid)
plot(lasso.fit)
x = model.matrix(price~., listings.train)
lasso.fit = glmnet(x, listings.train$price, alpha = 1, lambda = 2)
plot(lasso.fit)
cv.out=cv.glmnet(x, listings.train$price, alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
x.test = model.matrix(price~., listings.test)
lasso.pred=predict(lasso.fit,s=bestlam ,newx=x.test)
mean((lasso.pred-listings.test$price)^2)
lasso.coef=predict(lasso.fit,type="coefficients",s=bestlam)
lasso.coef
lasso.coef[lasso.coef!=0]
lasso.coef=predict(lasso.fit,type="coefficients",s=bestlam)[1:20, ]
lasso.coef
original = model.matrix(price~., listings)
out = glmnet(original, listings$price, alpha = 1, lambda = 2)
lasso.coef=predict(out,type="coefficients",s=bestlam)
lasso.coef
lasso.coef[lasso.coef!=0]
